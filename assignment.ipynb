{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368914c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb73d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Upload the PDF\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "pdf_path = list(uploaded.keys())[0]\n",
    "output_image_dir = \"pdf_images\"\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "extracted_content = []\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "    images_info = []\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        image_ext = base_image[\"ext\"]\n",
    "        image_filename = f\"page{page_num+1}_image{img_index+1}.{image_ext}\"\n",
    "        image_path = os.path.join(output_image_dir, image_filename)\n",
    "        \n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "        \n",
    "        images_info.append(image_path)\n",
    "    \n",
    "    extracted_content.append({\n",
    "        \"page\": page_num + 1,\n",
    "        \"text\": text.strip(),\n",
    "        \"images\": images_info\n",
    "    })\n",
    "\n",
    "# Save the structured content\n",
    "with open(\"structured_output.json\", \"w\") as f:\n",
    "    json.dump(extracted_content, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e93a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0344ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    raw_image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9ca704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "image_paths = glob.glob(\"pdf_images/*.png\")\n",
    "qa_list = []\n",
    "\n",
    "for img in image_paths:\n",
    "    caption = generate_caption(img)\n",
    "    question = f\"What is shown in the image: '{caption}'?\"\n",
    "    qa_list.append({\n",
    "        \"question\": question,\n",
    "        \"images\": img,\n",
    "        \"option_images\": []  # You can add if found later\n",
    "    })\n",
    "\n",
    "with open(\"ai_generated_questions.json\", \"w\") as f:\n",
    "    json.dump(qa_list, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "996eeb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "image_paths = glob.glob(\"pdf_images/*.png\")\n",
    "qa_list = []\n",
    "\n",
    "for img in image_paths:\n",
    "    caption = generate_caption(img)\n",
    "    question = f\"What is shown in the image: '{caption}'?\"\n",
    "    qa_list.append({\n",
    "        \"question\": question,\n",
    "        \"images\": img,\n",
    "        \"option_images\": []  # You can add if found later\n",
    "    })\n",
    "\n",
    "with open(\"ai_generated_questions.json\", \"w\") as f:\n",
    "    json.dump(qa_list, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb1cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "image_paths = glob.glob(\"pdf_images/*.png\")\n",
    "qa_list = []\n",
    "\n",
    "for img in image_paths:\n",
    "    caption = generate_caption(img)\n",
    "    question = f\"What is shown in the image: '{caption}'?\"\n",
    "    qa_list.append({\n",
    "        \"question\": question,\n",
    "        \"images\": img,\n",
    "        \"option_images\": []  # You can add if found later\n",
    "    })\n",
    "\n",
    "with open(\"ai_generated_questions.json\", \"w\") as f:\n",
    "    json.dump(qa_list, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff0afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ai_generated_questions.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(data[:5])  # View first 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b291da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ai_generated_questions.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(data[:5])  # View first 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4681bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''\n",
    "  <a href=\"javascript:window.print()\" target=\"_blank\">\n",
    "    <button style=\"padding:10px 20px;font-size:16px;margin-top:10px\">\n",
    "      ðŸ“„ Save as PDF\n",
    "    </button>\n",
    "  </a>\n",
    "''')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
